{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PacTimeOrig.data import DataHandling as dh\n",
    "from PacTimeOrig.data import DataProcessing as dp\n",
    "from PacTimeOrig.Attractor import base as attractor\n",
    "from PacTimeOrig.utils import processing as proc\n",
    "from ChangeOfMind.functions import processing as procdata\n",
    "from PacTimeOrig.models import io,NNtorch,training,utils,lossfn\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "\n",
    "\n",
    "def probabilities_to_logits_sigmoid(probabilities):\n",
    "    probabilities = np.clip(probabilities, 1e-15, 1 - 1e-15)  # Avoid log(0) or division by zero\n",
    "    logits = np.log(probabilities / (1 - probabilities))\n",
    "    return logits\n",
    "\n",
    "\n",
    "\n",
    "def replace_bad_logits(logits_list, clip_min=-1e3, clip_max=1e3):\n",
    "    for i, logit in enumerate(logits_list):\n",
    "        if not torch.isfinite(logit).all():  # Check for NaN or Inf\n",
    "            print(f\"Logit entry {i}: contains NaN or Inf values, replacing with clipped values\")\n",
    "            # Replace NaN or Inf with 0 as a fallback\n",
    "            logits_list[i] = torch.full_like(logit, 0)\n",
    "        else:\n",
    "            # Replace extreme values with clipped values\n",
    "            min_mask = logit < clip_min\n",
    "            max_mask = logit > clip_max\n",
    "\n",
    "            if min_mask.any() or max_mask.any():\n",
    "                print(f\"Logit entry {i}: min={logit.min()}, max={logit.max()} (extreme values detected)\")\n",
    "                \n",
    "                # Clip the logits\n",
    "                logit[min_mask] = clip_min\n",
    "                logit[max_mask] = clip_max\n",
    "                \n",
    "                # Use the sign of the last valid value, if available\n",
    "                for j in torch.where(min_mask | max_mask)[0]:\n",
    "                    if j > 0:  # Ensure there is a previous value\n",
    "                        logit[j] = torch.sign(logit[j - 1]) * torch.clamp(logit[j], clip_min, clip_max)\n",
    "                    else:\n",
    "                        logit[j] = clip_min if min_mask[j] else clip_max\n",
    "\n",
    "            logits_list[i] = logit\n",
    "\n",
    "    return logits_list\n",
    "\n",
    "\n",
    "\n",
    "def soft_range_normalize(tensor_list):\n",
    "    # Concatenate all tensors along a new dimension\n",
    "    all_data = torch.cat(tensor_list, dim=0)  # Shape: (total_samples, num_neurons)\n",
    "\n",
    "    # Compute mean and range for each neuron (across the column dimension)\n",
    "    neuron_means = all_data.mean(dim=0)  # Shape: (num_neurons,)\n",
    "    neuron_ranges = all_data.max(dim=0).values - all_data.min(dim=0).values  # Shape: (num_neurons,)\n",
    "    neuron_ranges_sqrt = torch.sqrt(neuron_ranges + 1e-8)  # Add epsilon for numerical stability\n",
    "\n",
    "    # Normalize each tensor in the list\n",
    "    normalized_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        # Subtract mean and divide by sqrt of range for each neuron\n",
    "        normalized_tensor = (tensor - neuron_means) / neuron_ranges_sqrt\n",
    "        normalized_tensors.append(normalized_tensor)\n",
    "\n",
    "    return normalized_tensors\n",
    "\n",
    "def compute_soft_range_params(tensor_list):\n",
    "    \"\"\"\n",
    "    Compute the mean and range parameters for soft range normalization.\n",
    "    \"\"\"\n",
    "    # Concatenate all tensors in the train set\n",
    "    all_data = torch.cat(tensor_list, dim=0)  # Shape: (total_samples, num_neurons)\n",
    "\n",
    "    # Compute mean and range for each neuron\n",
    "    neuron_means = all_data.mean(dim=0)  # Shape: (num_neurons,)\n",
    "    neuron_ranges = all_data.max(dim=0).values - all_data.min(dim=0).values  # Shape: (num_neurons,)\n",
    "    neuron_ranges_sqrt = torch.sqrt(neuron_ranges + 1e-8)  # Add epsilon for numerical stability\n",
    "\n",
    "    return neuron_means, neuron_ranges_sqrt\n",
    "\n",
    "\n",
    "def compute_zscore_params(tensor_list):\n",
    "    \"\"\"\n",
    "    Compute the mean and range parameters for soft range normalization.\n",
    "    \"\"\"\n",
    "    # Concatenate all tensors in the train set\n",
    "    all_data = torch.cat(tensor_list, dim=0)  # Shape: (total_samples, num_neurons)\n",
    "\n",
    "    # Compute mean and range for each neuron\n",
    "    neuron_means = all_data.mean(dim=0)  # Shape: (num_neurons,)\n",
    "    neuron_std = all_data.std(dim=0)   # Shape: (num_neurons,)\n",
    "\n",
    "    return neuron_means, neuron_std\n",
    "\n",
    "\n",
    "\n",
    "def apply_soft_range_normalization(tensor_list, neuron_means, neuron_ranges_sqrt):\n",
    "    \"\"\"\n",
    "    Apply soft range normalization using precomputed parameters.\n",
    "    \"\"\"\n",
    "    normalized_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        # Subtract mean and divide by sqrt of range for each neuron\n",
    "        normalized_tensor = (tensor - neuron_means) / neuron_ranges_sqrt\n",
    "        normalized_tensors.append(normalized_tensor)\n",
    "    return normalized_tensors\n",
    "\n",
    "\n",
    "\n",
    "def apply_zscore_normalization(tensor_list, neuron_means, neuron_std):\n",
    "    \"\"\"\n",
    "    Apply soft range normalization using precomputed parameters.\n",
    "    \"\"\"\n",
    "    normalized_tensors = []\n",
    "    for tensor in tensor_list:\n",
    "        # Subtract mean and divide by sqrt of range for each neuron\n",
    "        normalized_tensor = (tensor - neuron_means) / neuron_std\n",
    "        normalized_tensors.append(normalized_tensor)\n",
    "    return normalized_tensors\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prepare behaviora and neural data",
   "id": "51f15d5fe7476867"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cfgparams={}\n",
    "cfgparams['event']='zero' # 'zero\n",
    "cfgparams['keepamount']=40\n",
    "cfgparams['timewarp']={}\n",
    "cfgparams['prewin']=14\n",
    "cfgparams['behavewin']=15\n",
    "cfgparams['timewarp']['dowarp']=True\n",
    "cfgparams['timewarp']['warpN']=cfgparams['prewin']+cfgparams['behavewin']+1\n",
    "cfgparams['timewarp']['originalTimes']=np.arange(1,cfgparams['timewarp']['warpN']+1)\n",
    "cfgparams['percent_train']=0.9\n",
    "cfgparams['smoothing']=120\n",
    "\n",
    "vars_sess_H_pmd, psth_sess_H_pmd, Xd_sess_H_pmd, outputs_sess_H_pmd,cfg_H_pmd=procdata.get_all_vars_nhp(subj='H', area='dACC', sessions=np.arange(1,6),prewin=cfgparams['prewin'],behavewin=cfgparams['behavewin'])\n"
   ],
   "id": "b9f2fed7ea68e17c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Do by session",
   "id": "39d0fce5bbf0f25a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sess=2\n",
    "#Get Wt and remove is bad\n",
    "cfg={}\n",
    "cfg['subj']='H'\n",
    "cfg['session']=sess\n",
    "cfg['wtype']='bma'\n",
    "cfg['folder']='/Users/user/PycharmProjects/PacManMain/ChangeOfMind/results/NHP/'\n",
    "\n",
    "#Get wt\n",
    "wt=procdata.get_wt(cfg)\n",
    "\n",
    "# Only save trials in which Wt was good\n",
    "indices_to_keep = outputs_sess_H_pmd[sess]['is_good_index']\n",
    "wt = [item for idx, item in enumerate(wt) if idx in indices_to_keep]\n",
    "\n",
    "#Set inputs for RNN\n",
    "N_neurons=psth_sess_H_pmd[sess][0].shape[1]\n",
    "X_train=psth_sess_H_pmd[sess]\n",
    "X_train = [df.astype(float) for df in X_train]\n",
    "\n",
    "#making firing rates smoother\n",
    "sigma = procdata._gauss_smooth_parameters(80)\n",
    "\n",
    "X_train = [df.apply(lambda col: gaussian_filter1d(col, sigma=sigma), axis=0) for df in X_train]\n",
    "\n",
    "#Convert to arrays\n",
    "X_train = [np.array(df) for df in X_train]\n",
    "\n",
    "\n",
    "# wt = [probabilities_to_logits_sigmoid(arr) for arr in wt]\n",
    "wt = [torch.tensor(arr) for arr in wt]\n",
    "X_train = [torch.tensor(arr) for arr in X_train]\n",
    "# wt=replace_bad_logits(wt)\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=procdata.split_train_test(X_train, wt, test_size=0.2, seed=42)\n",
    "\n",
    "neuron_means, neuron_ranges_sqrt=compute_soft_range_params(X_train)\n",
    "\n",
    "\n",
    "X_train=apply_soft_range_normalization(X_train, neuron_means, neuron_ranges_sqrt)\n",
    "\n",
    "X_test=apply_soft_range_normalization(X_test, neuron_means, neuron_ranges_sqrt)\n",
    "\n",
    "\n",
    "mu,std = compute_zscore_params(Y_train)\n",
    "\n",
    "# Y_train = apply_zscore_normalization(Y_train,mu,std)\n",
    "# Y_test = apply_zscore_normalization(Y_test,mu,std)\n"
   ],
   "id": "b7a89434f3e2bd18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Try LSTM/GRU, higher hidden, logit or softmac",
   "id": "f7090e4b45fe32d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset = io.VariableLengthDataset(X_train, Y_train)\n",
    "dataloader = io.torchloader(dataset,batch_size=32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.BCELoss()\n",
    "model=NNtorch.RNN_decoder(input_size=N_neurons,hidden_size=64,output_size=1,use_softmax=True,rnn_type=\"simpleRNN\").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.15)\n",
    "training.train_masked_rnn(model,dataloader,criterion,optimizer,device,num_epochs=10,outcard=1)\n"
   ],
   "id": "6da89e2f4d273701"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "val_loss=[]\n",
    "r2=[]\n",
    "for i in range(len(X_test)):\n",
    "    X_data=X_test[i]\n",
    "    X_trial = torch.tensor(X_data)  # Your input data\n",
    "    X_trial = X_trial.float()  # Converts tensor to torch.float32\n",
    "    seq_length = torch.tensor([X_trial.shape[0]])\n",
    "    \n",
    "    X_trial = X_trial.unsqueeze(0)  # Shape: (1, seq_len, input_size)\n",
    "    # Move data to the appropriate device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    X_trial = X_trial.to(device)\n",
    "    seq_length = seq_length.to(device)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "    # Run the model\n",
    "        outputs = model(X_trial, seq_length)\n",
    "        # Remove batch dimension if needed\n",
    "        outputs = outputs.squeeze(0) \n",
    "    plt.plot(outputs)\n",
    "    r2.append(r2_score(Y_test[i].numpy().flatten(), outputs.numpy().flatten()))\n",
    "    val_loss.append(np.array(criterion(outputs,Y_test[i])))"
   ],
   "id": "75ba3afac0443fe9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
