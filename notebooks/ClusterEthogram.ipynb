{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Things to cluster on:\n",
    "Acceleration\n",
    "angular velocity\n",
    "accelartion towards target\n",
    "angular velocity towards target\n",
    "\n",
    "\n",
    "\n",
    "Distance\n",
    "relative position and its deriv\n",
    "velocity toward prey/pred == the velocity projection onto x-x and y-y, but without unit normalization, just dot product\n",
    "relative angle\n",
    "relative velocity\n",
    "rate of change of relative heading angle.\n",
    "self velocity.\n",
    "self accelartion.\n",
    "\n",
    "Cluster these things based on given time-windows.  How to choose dt, will save for later.\n",
    "#discern going and resting by velocity thresholds....\n",
    "\n",
    "# Methods:\n",
    "Affinity propagation split between trial types: single target, multi-prey, prey-pred.\n",
    "#need to get all sessions\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\Documents\\GitHub\\PacManHuman\\PacTimeOrig\\Methods\\utils.py:269: RuntimeWarning: invalid value encountered in true_divide\n",
      "  filt_speed = filt_speed / np.max(filt_speed)\n",
      " 33%|███▎      | 1/3 [00:28<00:57, 28.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:58<00:29, 29.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\Documents\\GitHub\\PacManHuman\\PacTimeOrig\\Methods\\utils.py:269: RuntimeWarning: invalid value encountered in true_divide\n",
      "  filt_speed = filt_speed / np.max(filt_speed)\n",
      "100%|██████████| 3/3 [01:24<00:00, 28.19s/it]\n"
     ]
    }
   ],
   "source": [
    "import PacTimeOrig.DataHandling as DH\n",
    "import PacTimeOrig.Methods.utils as pacutils\n",
    "from PacTimeOrig.Methods.Clusteresque import recusiveCDF, EMDRecursive,getvarforclustering,loadEMDmatrices,EMDfast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "#set some analysis variables that'll stay fixed\n",
    "winlength=10\n",
    "n_sessions=5\n",
    "for i in tqdm(np.arange(3,6,1)):\n",
    "    session=i\n",
    "\n",
    "    data=DH.dataloader(direc='C:\\\\Users\\\\Justin\\\\Documents\\\\GitHub\\\\PacManHuman\\\\',subject='H',session = session, suffix='Pac_dACC.mat')\n",
    "    sessionVars = DH.ExperimentVarsRetrieve(data)\n",
    "    positions = DH.retrievepositions(data)\n",
    "    #velocity, accelaration\n",
    "    positions = pacutils.computederivatives(positions)\n",
    "    #distance and angle\n",
    "    positions = pacutils.computeheading(positions)\n",
    "    pacutils.computevelocitytowardstarget(positions)\n",
    "\n",
    "    #Append the above info into SessionVars as a goodbad (good =1), get rection time, and idx of first peak for splitting\n",
    "    sessionVars=pacutils.trialRtPkRej(sessionVars, positions)\n",
    "\n",
    "\n",
    "    #Get aggregated features for each trial\n",
    "    p1trialdat=pacutils.getclusterfeatures(sessionVars, positions,winlength=winlength)\n",
    "\n",
    "    #Make sure not to include bad trials using the gb flag in sessionVars, let's make a new variable for it\n",
    "    p1trials = sessionVars[sessionVars['numNPC'] == 1].index\n",
    "    goodbad=sessionVars['goodtrial'].loc[p1trials].reset_index(drop=True)\n",
    "\n",
    "\n",
    "    alldat=dict()\n",
    "    alldat['positions']=positions\n",
    "    alldat['sessionVars']=sessionVars\n",
    "    alldat['p1trialdat']=p1trialdat\n",
    "    alldat['p1trials']=p1trials\n",
    "    alldat['goodbad']=goodbad\n",
    "    #Save the matrices for later analysis\n",
    "    import pickle\n",
    "\n",
    "    with open('C:\\\\Users\\\\Justin\\\\Documents\\\\GitHub\\\\PacManHuman\\\\Results\\\\datum'+str(session)+'.pkl', 'wb') as handle:\n",
    "        pickle.dump(alldat, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#include:\n",
    "1. subject speed\n",
    "2. subject sum(acceleration.^2)\n",
    "3. velocity\n",
    "4. acceleration\n",
    "5. velocity towards target -- as scalar projection\n",
    "6. rate of change of scalar projection\n",
    "6. relative angular heading\n",
    "7. rate of change of angular heading\n",
    "8. work or dot product of vector projection of velocity acceleration as the scalar projection onto the acceleration vector.\n",
    "9. relative target quantities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get speed distributions by target type (for normalization)\n",
    "unval=np.unique(sessionVars.loc[p1trials]['NPCvalA']) #Get unique values\n",
    "tmpval=sessionVars.loc[p1trials]['NPCvalA'] #Make PD df with just those values\n",
    "tmpval.reset_index(drop=True,inplace=True) #and reset its index for using np.where\n",
    "\n",
    "spdout=[]\n",
    "for i in unval:\n",
    "    tmptrial=np.where(tmpval==i)[0]\n",
    "    tmpspd=pd.DataFrame()\n",
    "    for trial in tmptrial:\n",
    "\n",
    "        tmpspd=pd.concat((tmpspd,pd.DataFrame(p1trialdat[trial].prey1_scalar_projection)),axis=0)\n",
    "\n",
    "    spdout.append(tmpspd)\n",
    "\n",
    "#make a kernel density plot\n",
    "sns.kdeplot(pd.DataFrame(spdout[0].to_numpy())[0])\n",
    "sns.kdeplot(pd.DataFrame(spdout[1].to_numpy())[0])\n",
    "sns.kdeplot(pd.DataFrame(spdout[2].to_numpy())[0])\n",
    "sns.kdeplot(pd.DataFrame(spdout[3].to_numpy())[0])\n",
    "sns.kdeplot(pd.DataFrame(spdout[4].to_numpy())[0])\n",
    "plt.ylabel('density')\n",
    "plt.xlabel('speed')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 3/17/23\n",
    "\n",
    "# Load each pikcled datum and concatante the P1 trials\n",
    "datall=dict()\n",
    "for i in range(5):\n",
    "    session=i+1\n",
    "    file = open('C:\\\\Users\\\\Justin\\\\Documents\\\\GitHub\\\\PacManHuman\\\\Results\\\\datum'+str(session)+'.pkl', 'rb')\n",
    "    emdmatrices = pickle.load(file)\n",
    "    datall[str(session)]=emdmatrices\n",
    "\n",
    "# Then compute recursive Earth movers across all and store matrices for clustering.\n",
    "#Fewer bins\n",
    "histrange=dict()\n",
    "\n",
    "histrange['speed']=np.arange(0,1400+300,400) # 4bins\n",
    "histrange['speedgrad']=np.arange(-200,200+50,100) # 4bins\n",
    "histrange['accelmag']=np.arange(0,20000+5000,5000) # 4bins\n",
    "histrange['prey1reldistance']=np.arange(0,2000+400,400) # 5bins\n",
    "histrange['prey1reldistancegrad']=np.arange(-40,40+10,20) #4 bins\n",
    "histrange['prey1relangularhead']= np.arange(-180,180+45,45)\n",
    "histrange['prey1relangularheadgrad']= np.arange(-30,30+30,15)\n",
    "histrange['prey1_scalar_projection']=np.arange(-1500,1500+500,500)\n",
    "histrange['prey1_scalar_projection_gradient']=np.arange(-200,200+100,100)\n",
    "\n",
    "emdmats=dict()\n",
    "\n",
    "allvars=['speed','speedgrad','accelmag','prey1reldistance','prey1reldistancegrad','prey1relangularhead','prey1relangularheadgrad','prey1_scalar_projection','prey1_scalar_projection_gradient']\n",
    "\n",
    "\n",
    "#Compute recursive EMD for all above variables. It's fast afuck now\n",
    "for varname in allvars: #Loop over variables\n",
    "    dattmp=[] #list of dataframes\n",
    "    for i in range(n_sessions): #loop over sessions and retrieve data\n",
    "        session=i+1\n",
    "        dat=getvarforclustering(datall[str(session)]['p1trialdat'],varname,winlength)\n",
    "        dattmp.append([pd.DataFrame(dat)])\n",
    "\n",
    "    #Now combine all sessions\n",
    "    datcombo=pd.DataFrame()\n",
    "\n",
    "    for i in range(n_sessions):\n",
    "        datcombo=pd.concat((datcombo,dattmp[i][0]),axis=1)\n",
    "\n",
    "    #Now compute CDFs as prep for wasserstein distance\n",
    "    recdf = recusiveCDF(datcombo.to_numpy(),histrange,varname=varname)\n",
    "    #Now compute earthmovers distance\n",
    "    emdmat=EMDfast(recdf,normalize=False)\n",
    "    emdmatrices[varname]=emdmat\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Do affinity clustering\n",
    "A=(emdmatrices['speed']*-1)/np.max(emdmatrices['speed'])\n",
    "B=(emdmatrices['speedgrad']*-1)/np.max(emdmatrices['speedgrad'])\n",
    "C=(emdmatrices['prey1_scalar_projection']*-1)/np.max(emdmatrices['prey1_scalar_projection'])\n",
    "D=(emdmatrices['prey1_scalar_projection_gradient']*-1)/np.max(emdmatrices['prey1_scalar_projection_gradient'])\n",
    "E=(emdmatrices['prey1relangularheadgrad']*-1)/np.max(emdmatrices['prey1relangularheadgrad'])\n",
    "\n",
    "F=(emdmatrices['prey1reldistance']*-1)/np.max(emdmatrices['prey1reldistance'])\n",
    "\n",
    "\n",
    "afmat=((A+B+C+D+E+F)/6)\n",
    "#afmat=((A+B+C+D+E)/5)\n",
    "\n",
    "af = AffinityPropagation(random_state=0,affinity='precomputed',damping=0.95).fit(afmat*-1)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Get data after clustering and group for mean, variance, and joint kernel density maps\n",
    "dattmp=[] #list of dataframes\n",
    "for i in range(n_sessions): #loop over sessions and retrieve data\n",
    "    session=i+1\n",
    "    dat=getvarforclustering(datall[str(session)]['p1trialdat'],varname,winlength)\n",
    "    dattmp.append([pd.DataFrame(dat)])\n",
    "\n",
    "#Now combine all sessions\n",
    "datcombo=pd.DataFrame()\n",
    "\n",
    "for i in range(n_sessions):\n",
    "    datcombo=pd.concat((datcombo,dattmp[i][0]),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#How to flatten out for plotting\n",
    "datcombo.to_numpy()[:,np.where(af.labels_==3)[0]].transpose().flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#cluster notes:\n",
    "Accelmag and speedgradient are correlated a lot to the extent that you take the absolute value of speedgradient. May prefer speedgradient because it is directional (faster or slower)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Predefine bins for each variable type (either as a sequence or an integer)\n",
    "#Does it matter about differing bins. This will kinda cause different resolutions....\n",
    "# histrange=dict()\n",
    "# histrange['speed']=np.arange(0,1400+100,200)\n",
    "# histrange['speedgrad']=np.arange(-200,200+50,50)\n",
    "# histrange['accelmag']=np.arange(0,20000+5000,5000)\n",
    "# histrange['prey1reldistance']=np.arange(0,2000+200,200)\n",
    "# histrange['prey1reldistancegrad']=np.arange(-40,40+10,20)\n",
    "# histrange['prey1relangularhead']= np.arange(-180,180+45,45)\n",
    "# histrange['prey1relangularheadgrad']= np.arange(-30,30+30,15)\n",
    "# histrange['prey1_scalar_projection']=np.arange(-1500,1500+200,200)\n",
    "# histrange['prey1_scalar_projection_gradient']=np.arange(-200,200+100,100)\n",
    "\n",
    "#Fewer bins\n",
    "histrange=dict()\n",
    "\n",
    "histrange['speed']=np.arange(0,1400+300,400) # 4bins\n",
    "histrange['speedgrad']=np.arange(-200,200+50,100) # 4bins\n",
    "histrange['accelmag']=np.arange(0,20000+5000,5000) # 4bins\n",
    "histrange['prey1reldistance']=np.arange(0,2000+400,400) # 5bins\n",
    "histrange['prey1reldistancegrad']=np.arange(-40,40+10,20) #4 bins\n",
    "histrange['prey1relangularhead']= np.arange(-180,180+45,45)\n",
    "histrange['prey1relangularheadgrad']= np.arange(-30,30+30,15)\n",
    "histrange['prey1_scalar_projection']=np.arange(-1500,1500+500,500)\n",
    "histrange['prey1_scalar_projection_gradient']=np.arange(-200,200+100,100)\n",
    "\n",
    "\n",
    "emdmatrices=dict()\n",
    "\n",
    "allvars=['speed','speedgrad','accelmag','prey1reldistance','prey1reldistancegrad','prey1relangularhead','prey1relangularheadgrad','prey1_scalar_projection','prey1_scalar_projection_gradient']\n",
    "\n",
    "#Compute recursive EMD for all above variables. It'll take a bit\n",
    "for varname in allvars:\n",
    "    #Make vectors of each variable and then reshape (turn into function)\n",
    "    dat=getvarforclustering(p1trialdat,varname,winlength)\n",
    "    recdf=recusiveCDF(dat,histrange,varname=varname)\n",
    "    #Recursively compute earth movers distance. I could save time by recognizing it's symmetric and only compute unique entries (but I am lazy)..\n",
    "    emdmat=EMDRecursive(recdf,normalize=False)\n",
    "    emdmatrices[varname]=emdmat\n",
    "\n",
    "\n",
    "\n",
    "#Save the matrices for later analysis\n",
    "import pickle\n",
    "\n",
    "with open('C:\\\\Users\\\\Justin\\\\Documents\\\\GitHub\\\\PacManHuman\\\\Results\\\\emdmats10.pkl', 'wb') as handle:\n",
    "    pickle.dump(emdmatrices, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#normalize range of earth-movers to be [-1,0]\n",
    "emdmatrices=loadEMDmatrices('C:\\\\Users\\\\Justin\\\\Documents\\\\GitHub\\\\PacManHuman\\\\Results\\\\emdmats10.pkl')\n",
    "\n",
    "varsinclude=['speed','speedgrad','prey1_scalar_projection','accelmag','prey1_scalar_projection_gradient','prey1reldistance','prey1relangularhead','prey1relangularheadgrad']\n",
    "\n",
    "A=(emdmatrices['speed']*-1)/np.max(emdmatrices['speed'])\n",
    "B=(emdmatrices['speedgrad']*-1)/np.max(emdmatrices['speedgrad'])\n",
    "C=(emdmatrices['prey1_scalar_projection']*-1)/np.max(emdmatrices['prey1_scalar_projection'])\n",
    "D=(emdmatrices['prey1_scalar_projection_gradient']*-1)/np.max(emdmatrices['prey1_scalar_projection_gradient'])\n",
    "E=(emdmatrices['prey1relangularheadgrad']*-1)/np.max(emdmatrices['prey1relangularheadgrad'])\n",
    "\n",
    "F=(emdmatrices['prey1reldistance']*-1)/np.max(emdmatrices['prey1reldistance'])\n",
    "\n",
    "\n",
    "afmat=((A+B+C+D+E+F)/6)\n",
    "afmat=((A+B+C+D+E)/5)\n",
    "\n",
    "af = AffinityPropagation(random_state=0,affinity='precomputed',damping=0.95).fit(afmat*-1)\n",
    "\n",
    "#mean variables now to make look at functional behavior based on clusters\n",
    "behavecluster=dict() #Save these in a dictionary\n",
    "for varsget in varsinclude:\n",
    "    df=pd.DataFrame()\n",
    "    for i in np.unique(af.labels_):\n",
    "        dat=getvarforclustering(p1trialdat,varsget,winlength)\n",
    "        tmp=pd.DataFrame((dat[:,np.where(af.labels_==i)]).reshape((winlength,len(np.where(af.labels_==i)[0]))).mean(axis=1))\n",
    "        tmp.rename(columns={0:i},inplace=True)\n",
    "        df=pd.concat((df,tmp),axis=1)\n",
    "    behavecluster[varsget]=df\n",
    "\n",
    "\n",
    "varsget='prey1reldistance'\n",
    "dat=getvarforclustering(p1trialdat,varsget,winlength)\n",
    "for i in range(5):\n",
    "    sns.kdeplot(pd.DataFrame((dat[:, np.where(af.labels_ == i)]).reshape(\n",
    "        (winlength, len(np.where(af.labels_ == i)[0])))).to_numpy().flatten())\n",
    "\n",
    "\n",
    "#Build a 2d density plot with kdeplot\n",
    "varsget='prey1reldistance'\n",
    "dat=getvarforclustering(p1trialdat,varsget,winlength)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "#UMAP plot with cluster labels\n",
    "reducer = umap.UMAP(metric='precomputed',n_components=3,n_neighbors=25)\n",
    "embedding=reducer.fit_transform(afmat*-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "labels = hdbscan.HDBSCAN(\n",
    "    min_samples=20,\n",
    "    min_cluster_size=100,\n",
    ").fit_predict(embedding)\n",
    "clustered = (labels >= 0)\n",
    "plt.scatter(embedding[clustered, 0],\n",
    "            embedding[clustered, 1],\n",
    "            c=labels[clustered],\n",
    "            s=0.1,\n",
    "            cmap='Spectral')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#mean variables now to make look at functional behavior based on clusters\n",
    "behavecluster=dict() #Save these in a dictionary\n",
    "for varsget in varsinclude:\n",
    "    df=pd.DataFrame()\n",
    "    for i in np.unique(labels):\n",
    "        dat=getvarforclustering(p1trialdat,varsget,winlength)\n",
    "        tmp=pd.DataFrame((dat[:,np.where(labels==i)]).reshape((winlength,len(np.where(labels==i)[0]))).mean(axis=1))\n",
    "        tmp.rename(columns={0:i},inplace=True)\n",
    "        df=pd.concat((df,tmp),axis=1)\n",
    "    behavecluster[varsget]=df\n",
    "\n",
    "#Compute transition probability, now (could do as a GLM to include condition informtion and neural data later).\n",
    "#Get the probability distributions of variables for a given cluster\n",
    "\n",
    "#Do certain clusters occur at certain trial points"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Aggregate variables now to make look at functional behavior based on clusers\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Compute transition probability, now (could do as a GLM to include condition informtion and neural data later)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "A=(emdmatrices['speed']*-1)/np.max(emdmatrices['speed'])\n",
    "B=(emdmatrices['speedgrad']*-1)/np.max(emdmatrices['speedgrad'])\n",
    "C=(emdmatrices['prey1_scalar_projection']*-1)/np.max(emdmatrices['prey1_scalar_projection'])\n",
    "E=(emdmatrices['prey1_scalar_projection_gradient']*-1)/np.max(emdmatrices['prey1_scalar_projection_gradient'])\n",
    "F=(emdmatrices['prey1reldistance']*-1)/np.max(emdmatrices['prey1reldistance'])\n",
    "\n",
    "D=(emdmatrices['prey1reldistancegrad']*-1)/np.max(emdmatrices['prey1reldistancegrad'])\n",
    "\n",
    "afmat=((A+B+C+D+E+F)/6)\n",
    "\n",
    "#Let's try dim reduction/manifold learning first, then cluster\n",
    "from sklearn.manifold import TSNE\n",
    "import hdbscan\n",
    "\n",
    "projection = TSNE(metric='precomputed',perplexity=30).fit_transform(afmat*-1)\n",
    "\n",
    "labels = hdbscan.HDBSCAN(\n",
    "    min_samples=20,\n",
    "    min_cluster_size=30,\n",
    ").fit_predict(projection)\n",
    "\n",
    "clustered = (labels >= 0)\n",
    "\n",
    "plt.scatter(projection[clustered, 0],\n",
    "            projection[clustered, 1],\n",
    "            c=labels[clustered],\n",
    "            s=0.1,\n",
    "            cmap='Spectral')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "varsinclude=['speed','speedgrad','prey1_scalar_projection','prey1_scalar_projection_gradient','prey1reldistance','prey1reldistancegrad']\n",
    "\n",
    "\n",
    "#mean variables now to make look at functional behavior based on clusters\n",
    "behavecluster=dict() #Save these in a dictionary\n",
    "for varsget in varsinclude:\n",
    "    df=pd.DataFrame()\n",
    "    for i in np.unique(labels):\n",
    "        dat=getvarforclustering(p1trialdat,varsget,winlength)\n",
    "        tmp=pd.DataFrame((dat[:,np.where(labels==i)]).reshape((winlength,len(np.where(labels==i)[0]))).mean(axis=1))\n",
    "        tmp.rename(columns={0:i},inplace=True)\n",
    "        df=pd.concat((df,tmp),axis=1)\n",
    "    behavecluster[varsget]=df"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
